{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision import datasets,transforms\n",
    "from sklearn.metrics import f1_score\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sb\n",
    "import pandas as pd\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loding dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_data():\n",
    "    #Define transformation\n",
    "\n",
    "    transform= transforms.ToTensor()\n",
    "\n",
    "    #Load the training dataset\n",
    "\n",
    "    train_dataset= datasets.FashionMNIST(root='./data',train=True,download=True,transform=transform)\n",
    "\n",
    "    #Load the test dataset separately\n",
    "\n",
    "    test_dataset= datasets.FashionMNIST(root='./data',train=False,download=True,transform=transform)\n",
    "    return train_dataset,test_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(train_dataset,test_dataset):\n",
    "\n",
    "    n,sizex,sizey=train_dataset.data.shape\n",
    "    train_data=train_dataset.data.reshape(n,sizex*sizey)\n",
    "    train_data=train_data.numpy()\n",
    "    n,sizex,sizey=test_dataset.data.shape\n",
    "    test_data=test_dataset.data.reshape(n,sizex*sizey)\n",
    "    test_data=test_data.numpy()\n",
    "    # train_data=[]\n",
    "    # for data in train_dataset.data:\n",
    "    #     new_data=np.array(data.numpy().flatten())\n",
    "    #     train_data.append(new_data)\n",
    "    \n",
    "\n",
    "    # test_data=[]\n",
    "    # for data in test_dataset.data:\n",
    "    #     new_data=np.array(data.numpy().flatten())\n",
    "    #     test_data.append(new_data)\n",
    "    return train_data,test_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########    spliting   ##########\n",
    "from sklearn.model_selection import train_test_split\n",
    "def split(train_data,train_target):\n",
    "    train_data, val_data, train_target, val_target = train_test_split(train_data, train_target, test_size=0.2, random_state=42)\n",
    "    return train_data, val_data, train_target, val_target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "########   one hot encoding  ########\n",
    "def one_hot_encoding(y):\n",
    "    n_values = np.max(y) + 1\n",
    "    return np.eye(n_values)[y]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### cross entropy loss ##########\n",
    "def CategoricalCrossEntropy(y_true, y_pred):\n",
    "    epsilon = 1e-11\n",
    "    y_pred = np.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    return -np.sum(y_true * np.log(y_pred))/len(y_true)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################   Adam optimizer   ################\n",
    "def adam_optimizer(output_grad,m_t,v_t,t,w,learning_rate,beta1,beta2,epsilon):\n",
    "    m = beta1 * m_t + (1 - beta1) * output_grad\n",
    "    v = beta2 * v_t + (1 - beta2) * output_grad**2\n",
    "    m_hat = m / (1 - beta1**t)\n",
    "    v_hat = v / (1 - beta2**t)\n",
    "    # print(t)\n",
    "    w = w - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "    return w , m , v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "################  Dropout layer  ################\n",
    "class Dropout:\n",
    "    def __init__(self,dropout_rate):\n",
    "        self.dropout_rate=dropout_rate\n",
    "        self.mask=None\n",
    "    def forward(self,X,trainig=True):\n",
    "        if trainig:\n",
    "            self.mask=np.random.binomial(1,1-self.dropout_rate,X.shape)/(1-self.dropout_rate)\n",
    "            return X*self.mask\n",
    "        else:\n",
    "            return X\n",
    "    def backward(self,grad,learning_rate):\n",
    "        return grad*self.mask\n",
    "    def clear(self):\n",
    "        self.mask=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class batchNormalization:\n",
    "    def __init__(self,input_size):\n",
    "        self.gamma = np.ones(input_size)\n",
    "        self.beta = np.zeros(input_size)\n",
    "        self.gamma_m=np.zeros(input_size)\n",
    "        self.beta_m=np.zeros(input_size)\n",
    "        self.gamma_v=np.zeros(input_size)\n",
    "        self.beta_v=np.zeros(input_size)\n",
    "        self.beta1=0.9\n",
    "        self.beta2=0.999\n",
    "        self.epsilon=1e-8\n",
    "        self.t=1\n",
    "\n",
    "        self.running_mean = np.zeros(input_size)\n",
    "        self.running_var = np.ones(input_size)\n",
    "        self.momentum = 0.9\n",
    "    \n",
    "    def forward(self,x,training=True):\n",
    "        \n",
    "        if training==True:\n",
    "            self.input = x\n",
    "            self.mean = np.mean(x,axis=0)\n",
    "            self.var = np.var(x,axis=0)\n",
    "            self.x_hat = (x - self.mean) / np.sqrt(self.var + self.epsilon)\n",
    "            self.y = self.gamma * self.x_hat + self.beta\n",
    "\n",
    "            # Update running statistics for inference\n",
    "            self.running_mean = (self.momentum * self.running_mean +\n",
    "                                (1 - self.momentum) * self.mean)\n",
    "            self.running_var = (self.momentum * self.running_var +\n",
    "                                (1 - self.momentum) * self.var)\n",
    "        else:\n",
    "            self.x_hat = (x - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
    "            self.y = self.gamma * self.x_hat + self.beta\n",
    "        return self.y\n",
    "    def backward(self,grad,learning_rate):\n",
    "        m = grad.shape[0]\n",
    "        self.dgamma = np.sum(grad * self.x_hat,axis=0)\n",
    "        self.dbeta = np.sum(grad,axis=0)\n",
    "        \n",
    "        dx_hat = grad * self.gamma\n",
    "        dvar = np.sum(dx_hat * (self.input - self.mean) * -0.5 * (self.var + self.epsilon)**-1.5,axis=0)\n",
    "        dmean = np.sum(dx_hat * -1 / np.sqrt(self.var + self.epsilon),axis=0) + dvar * np.sum(-2 * (self.input - self.mean),axis=0) / m\n",
    "        dx = dx_hat / np.sqrt(self.var + self.epsilon) + dvar * 2 * (self.input - self.mean) / m + dmean / m\n",
    "        self.gamma, self.gamma_m, self.gamma_v = adam_optimizer(self.dgamma,self.gamma_m,self.gamma_v,self.t,self.gamma,learning_rate,self.beta1,self.beta2,self.epsilon)\n",
    "        self.beta, self.beta_m, self.beta_v = adam_optimizer(self.dbeta,self.beta_m,self.beta_v,self.t,self.beta,learning_rate,self.beta1,self.beta2,self.epsilon)\n",
    "        self.t+=1\n",
    "        return dx\n",
    "\n",
    "    def clear(self):\n",
    "        self.input=None\n",
    "        self.mean=None\n",
    "        self.var=None\n",
    "        self.dgamma=None\n",
    "        self.dbeta=None\n",
    "        self.x_hat=None\n",
    "        self.y=None\n",
    "        self.gamma_m=None\n",
    "        self.beta_m=None\n",
    "        self.gamma_v=None\n",
    "        self.beta_v=None\n",
    "        self.beta1=None\n",
    "        self.beta2=None\n",
    "        self.t=None\n",
    "        self.momentum = None\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMax:\n",
    "    def forward(self, x):\n",
    "        exps = np.exp(x-np.max(x, axis=1, keepdims=True))\n",
    "        self.activation=exps / np.sum(exps, axis=1, keepdims=True)\n",
    "        return self.activation\n",
    "    def backward(self,y_actual,learning_rate):\n",
    "        return self.activation-y_actual  ### dL/dz= A[level]-y\n",
    "    def clear(self):\n",
    "        self.activation=None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########  ReLU  ##########\n",
    "class ReLU:\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        return np.maximum(0, x)\n",
    "    def backward(self,output_gradient,learning_rate):\n",
    "        return np.multiply(output_gradient,np.where(self.input>0,1,0))\n",
    "    def clear(self):\n",
    "        self.input=None\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### dense layer ########\n",
    "class dense:\n",
    "    def __init__(self, input_size, output_size):\n",
    "\n",
    "        ###momentum and variance initialization\n",
    "        self.m_weights = np.zeros((input_size, output_size))\n",
    "        self.m_bias = np.zeros(output_size)\n",
    "        self.v_weights = np.zeros((input_size, output_size))\n",
    "        self.v_bias = np.zeros(output_size)\n",
    "        self.beta1=0.9\n",
    "        self.beta2=0.999\n",
    "        self.epsilon=1e-8\n",
    "        self.t=1\n",
    "        # Xavier Initialization\n",
    "        self.weights = np.random.randn(input_size, output_size) * np.sqrt(2. / (input_size + output_size))\n",
    "        self.bias = np.random.randn(output_size)\n",
    "        # self.weights = np.ones((input_size, output_size))\n",
    "        # self.bias = np.ones(output_size)\n",
    "        # print(self.weights)\n",
    "        # print(self.bias)\n",
    "      \n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        self.output = np.dot(input, self.weights) + self.bias\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self ,output_gradient, learning_rate):\n",
    "        \n",
    "        backward_gradient = np.dot(output_gradient, self.weights.T)/output_gradient.shape[1]\n",
    "        weights_gradient = np.dot(self.input.T, output_gradient)/output_gradient.shape[0]\n",
    "        bias_gradient = np.mean(output_gradient, axis=0)\n",
    "        self.weights,self.m_weights,self.v_weights =adam_optimizer(weights_gradient,self.m_weights,self.v_weights,self.t,self.weights,learning_rate,self.beta1,self.beta2,self.epsilon)\n",
    "        self.bias,self.m_bias,self.v_bias =adam_optimizer(bias_gradient,self.m_bias,self.v_bias,self.t,self.bias,learning_rate,self.beta1,self.beta2,self.epsilon)\n",
    "        self.t+=1\n",
    "\n",
    "        \n",
    "        return backward_gradient\n",
    "    def clear(self):\n",
    "        self.input=None\n",
    "        self.output=None\n",
    "        self.m_weights=None\n",
    "        self.m_bias=None\n",
    "        self.v_weights=None\n",
    "        self.v_bias=None\n",
    "        self.beta1=None\n",
    "        self.beta2=None\n",
    "        self.epsilon=None\n",
    "        self.t=None\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fnn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN:\n",
    "    def __init__(self,train_X,train_Y,val_X,val_Y):\n",
    "        self.train_X=train_X\n",
    "        self.train_Y=train_Y\n",
    "        self.val_X=val_X\n",
    "        self.val_Y=val_Y\n",
    "    \n",
    "    def train(self,model,epochs,batch_size,learning_rate):\n",
    "\n",
    "        training_loss=[]\n",
    "        validation_loss=[]\n",
    "        training_accuracy=[]\n",
    "        validation_accuracy=[]\n",
    "        validation_f1_score=[]\n",
    "        best_f1_score=0\n",
    "        best_model=model\n",
    "        for i in range(epochs):\n",
    "\n",
    "            train_loss=0\n",
    "            val_loss=0\n",
    "            train_accuracy=0\n",
    "            val_accuracy=0\n",
    "            total=0\n",
    "            indices=np.random.permutation(self.train_X.shape[0])\n",
    "            self.train_X=self.train_X[indices]\n",
    "            self.train_Y=self.train_Y[indices]\n",
    "            batch_count=0\n",
    "            for j in range(0,self.train_X.shape[0],batch_size):\n",
    "                batch_X= np.array(self.train_X[j:j+batch_size])\n",
    "                batch_Y=np.array(self.train_Y[j:j+batch_size])\n",
    "                output=batch_X\n",
    "                for layer in model:\n",
    "                    if isinstance(layer,batchNormalization):\n",
    "                        output=layer.forward(output,training=True)\n",
    "                    else:\n",
    "                        output=layer.forward(output)\n",
    "                \n",
    "                loss=CategoricalCrossEntropy(batch_Y,output)\n",
    "                batch_count+=1\n",
    "                train_loss+=loss\n",
    "                accuracy=np.sum(np.argmax(output,axis=1)==np.argmax(batch_Y,axis=1))\n",
    "                train_accuracy+=accuracy\n",
    "                total+=len(batch_Y)\n",
    "                output_grad=batch_Y\n",
    "                for layer in reversed(model):\n",
    "                    output_grad=layer.backward(output_grad,learning_rate)\n",
    "            \n",
    "            training_loss.append(train_loss/batch_count)\n",
    "            training_accuracy.append(train_accuracy/total)\n",
    "            output=np.array(self.val_X)\n",
    "            for layer in model:\n",
    "                if isinstance(layer,batchNormalization) :\n",
    "                    output=layer.forward(output,training=False)\n",
    "                elif isinstance(layer,Dropout):\n",
    "                    output=layer.forward(output,trainig=False)\n",
    "                else:\n",
    "                    output=layer.forward(output)\n",
    "            val_loss=CategoricalCrossEntropy(self.val_Y,output)\n",
    "            validation_loss.append(val_loss)\n",
    "            val_accuracy=np.sum(np.argmax(output,axis=1)==np.argmax(self.val_Y,axis=1))\n",
    "            validation_accuracy.append(val_accuracy/len(self.val_Y))\n",
    "            validation_f1=f1_score(np.argmax(self.val_Y,axis=1),np.argmax(output,axis=1),average='macro')  \n",
    "            validation_f1_score.append(validation_f1) \n",
    "            # print(\"Epoch: \",i,\" Training Loss: \",train_loss/batch_count,\" Validation Loss: \",val_loss,\" Training Accuracy: \",train_accuracy/total,\" Validation Accuracy: \",val_accuracy/len(self.val_Y),\"validation_f1_Score: \",validation_f1)\n",
    "            if(validation_f1 > best_f1_score):\n",
    "                best_f1_score=validation_f1\n",
    "                best_model=model\n",
    "       \n",
    "        \n",
    "        \n",
    "        plt.plot(training_loss,label='Training Loss')\n",
    "        plt.plot(validation_loss,label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.text(15,(max(training_loss)+min(training_loss))/2,'learning rate: %s' %(learning_rate))\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        plt.plot(training_accuracy,label='Training Accuracy')\n",
    "        plt.plot(validation_accuracy,label='Validation Accuracy')\n",
    "        plt.text(15,(max(training_accuracy)+min(training_accuracy))/2,'learning rate: %s' %(learning_rate))\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(validation_f1_score,label='Validation F1 Score')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.text(15,(max(validation_f1_score)+min(validation_f1_score))/2,'learning rate: %s' %(learning_rate))\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "       \n",
    "        ## confusion matrix\n",
    "        \n",
    "        output=np.array(self.val_X)\n",
    "        for layer in best_model:\n",
    "            if isinstance(layer,batchNormalization):\n",
    "                output=layer.forward(output,training=False)\n",
    "            elif isinstance(layer,Dropout):\n",
    "                output=layer.forward(output,trainig=False)\n",
    "            else:\n",
    "                output=layer.forward(output)\n",
    "        y_pred=np.argmax(output,axis=1)\n",
    "        y_true=np.argmax(self.val_Y,axis=1)\n",
    "        con_m=confusion_matrix(y_true,y_pred)\n",
    "        # print(con_m)\n",
    "        con_m=pd.DataFrame(con_m,index=[\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"],\n",
    "                           columns=[\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"])\n",
    "        plt.figure(figsize=(10,7))\n",
    "        sb.heatmap(con_m,annot=True,fmt='d')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        return best_model\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "          \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(type):\n",
    "    model=[]\n",
    "    # if type==0:\n",
    "    #     model.append(dense(784,128))\n",
    "    #     model.append(ReLU())\n",
    "    #     model.append(dense(128,64))\n",
    "    #     model.append(ReLU())\n",
    "    #     model.append(dense(64,10))\n",
    "    #     model.append(SoftMax())\n",
    "    if type==0:\n",
    "        model.append(dense(784,128))\n",
    "        model.append(batchNormalization(128))\n",
    "        model.append(ReLU())\n",
    "        model.append(dense(128,64))\n",
    "        model.append(batchNormalization(64))\n",
    "        model.append(ReLU())\n",
    "        model.append(dense(64,10))\n",
    "        model.append(SoftMax())\n",
    "    elif type==1:\n",
    "        model.append(dense(784,128))\n",
    "        model.append(batchNormalization(128))\n",
    "        model.append(ReLU())\n",
    "        model.append(Dropout(0.2))\n",
    "        model.append(dense(128,64))\n",
    "        model.append(batchNormalization(64))\n",
    "        model.append(ReLU())\n",
    "        model.append(Dropout(0.2))\n",
    "        model.append(dense(64,10))\n",
    "        model.append(SoftMax())\n",
    "    elif type==2:\n",
    "        model.append(dense(784,256))\n",
    "        model.append(batchNormalization(256))\n",
    "        model.append(ReLU())\n",
    "        model.append(Dropout(0.3))\n",
    "        model.append(dense(256,128))\n",
    "        model.append(batchNormalization(128))\n",
    "        model.append(ReLU())\n",
    "        model.append(Dropout(0.2))\n",
    "        model.append(dense(128,64))\n",
    "        model.append(batchNormalization(64))\n",
    "        model.append(ReLU())\n",
    "        model.append(Dropout(0.2))\n",
    "        model.append(dense(64,10))\n",
    "        model.append(SoftMax())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FashionMNIST():\n",
    "    train_dataset,test_dataset=load_data()\n",
    "    train_data,test_data=preprocess(train_dataset,test_dataset)\n",
    "    train_targets=train_dataset.targets.numpy()\n",
    "    test_targets=test_dataset.targets.numpy()\n",
    "    X_train,X_val,y_train,y_val=split(train_data,train_targets)\n",
    "    y_train=one_hot_encoding(y_train)\n",
    "    y_val=one_hot_encoding(y_val)\n",
    "    y_test=one_hot_encoding(test_targets)\n",
    "    X_train=X_train/255\n",
    "    X_val=X_val/255\n",
    "    test_data=test_data/255\n",
    "    \n",
    "    fnn=FNN(X_train,y_train,X_val,y_val)\n",
    "\n",
    "    learning_rate=[0.005,0.001,0.0005,0.0001]\n",
    "    model_number=3\n",
    "    best_models=[]\n",
    "    for i in range(model_number):\n",
    "        for lr in learning_rate:\n",
    "            best_models.append(fnn.train(get_model(i),30,256,lr))\n",
    "\n",
    "    best_model=None\n",
    "    best_f1_score=0\n",
    "\n",
    "    for model in best_models:\n",
    "        output=np.array(X_val)\n",
    "        for layer in model:\n",
    "            if isinstance(layer,batchNormalization):\n",
    "                output=layer.forward(output,training=False)\n",
    "            elif isinstance(layer,Dropout):\n",
    "                output=layer.forward(output,trainig=False)\n",
    "            else:\n",
    "                output=layer.forward(output)\n",
    "        val_accuracy=np.sum(np.argmax(output,axis=1)==np.argmax(y_val,axis=1))\n",
    "        val_f1=f1_score(np.argmax(y_val,axis=1),np.argmax(output,axis=1),average='macro')  \n",
    "        if(val_f1>best_f1_score):\n",
    "            best_f1_score=val_f1\n",
    "            best_model=model\n",
    "        # print(\"\\n\")\n",
    "        # print(\"validation Accuracy: \",val_accuracy/len(y_val),\"validation_f1_Score: \",val_f1)\n",
    "\n",
    "         \n",
    "    return best_model,test_data,y_test\n",
    "    \n",
    "            \n",
    "\n",
    "\n",
    "########## uncomment this to train the model ###########\n",
    "\n",
    "# best_model,test_data,y_test=FashionMNIST()\n",
    "# print(\"Best Model: \",best_model)\n",
    "\n",
    "#### save in pickle file\n",
    "# for layer in best_model:\n",
    "#     layer.clear()\n",
    "# import pickle\n",
    "# with open('1905062_model.pkl','wb') as f:\n",
    "#     pickle.dump(best_model,f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###### Read pickle file ######\n",
    "\n",
    "# print(\"Testing\")\n",
    "# with open('1905062_model.pkl','rb') as f:\n",
    "#     model=pickle.load(f)\n",
    "\n",
    "# output=np.array(test_data)\n",
    "# for layer in model:\n",
    "    \n",
    "#     if isinstance(layer,batchNormalization):\n",
    "#         output=layer.forward(output,training=False)\n",
    "#     elif isinstance(layer,Dropout):\n",
    "#         output=layer.forward(output,trainig=False)\n",
    "#     else:\n",
    "#         output=layer.forward(output)\n",
    "# test_accuracy=np.sum(np.argmax(output,axis=1)==np.argmax(y_test,axis=1))\n",
    "# test_f1=f1_score(np.argmax(y_test,axis=1),np.argmax(output,axis=1),average='macro')\n",
    "# print(\"Testing Accuracy: \",test_accuracy/len(y_test),\"Testing_f1_Score: \",test_f1)\n",
    "# con_m=confusion_matrix(np.argmax(y_test,axis=1),np.argmax(output,axis=1))\n",
    "# con_m=pd.DataFrame(con_m,index=[\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"],\n",
    "#                            columns=[\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"])\n",
    "# plt.figure(figsize=(10,7))\n",
    "# sb.heatmap(con_m,annot=True,fmt='d')\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('True')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
